1.包安装问题：
flah-attn可能出现的问题就是libcuda.so链接问题，一般设置软链接和环境变量即可，详见“一些Python包”章节；
TransformerEngine和Apex未安装成功，不过貌似不影响运行。

2.关于在不支持flash-attn的卡上运行quickStart（即verl.trainer.main_ppo.py）的问题：
运行的卡为RTX5000(16GB)*2；Ubuntu版本为20.04；nvidia-smi信息为NVIDIA-SMI 570.133.20，Driver Version: 570.133.20，CUDA Version: 12.8；用户态CUDA版本为12.6；
用模块化方式启动；
脚本参数为：
data.train_files=../../data/processed_data/gsm8k/train.parquet
data.val_files=../../data/processed_data/gsm8k/test.parquet
data.train_batch_size=256
data.max_prompt_length=256
data.max_response_length=256
actor_rollout_ref.model.path=/home/wangweidong/model/Qwen3-0.6B
actor_rollout_ref.actor.optim.lr=1e-6
actor_rollout_ref.actor.ppo_mini_batch_size=64
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4
actor_rollout_ref.rollout.name=vllm
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8
actor_rollout_ref.rollout.tensor_model_parallel_size=1
actor_rollout_ref.rollout.gpu_memory_utilization=0.6
actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4
critic.optim.lr=1e-5
critic.model.path=/home/wangweidong/model/Qwen3-0.6B
critic.ppo_micro_batch_size_per_gpu=4
algorithm.kl_ctrl.kl_coef=0.001
trainer.logger=console
trainer.val_before_train=False
trainer.n_gpus_per_node=2
trainer.nnodes=1
trainer.save_freq=10
trainer.test_freq=10
trainer.total_epochs=15
# 此处开始为自己添加的参数
actor_rollout_ref.actor.fsdp_config.dtype=float16
actor_rollout_ref.rollout.dtype=float16
actor_rollout_ref.ref.fsdp_config.dtype=float16
actor_rollout_ref.rollout.max_model_len=1024
+nproc_per_node=2
actor_rollout_ref.rollout.max_num_seqs=512
actor_rollout_ref.rollout.max_num_batched_tokens=1024
+actor_rollout_ref.model.override_config.attn_implementation=eager
+critic.model.override_config.attn_implementation=eager
修改dtype是因为卡的内存不够，max_model_len不确定是否生效，和max_prompt_length、max_response_length、max_num_seqs、max_num_batched_tokens共同用于限制token长度以免内存不够；nproc_per_node设置使用2张卡；
actor_rollout_ref.model.override_config.attn_implementation和critic.model.override_config.attn_implementation用于设置_attn_implementation和attn_implementation，默认为flash_attn 2；
然而经过测试，critic.model.override_config.attn_implementation并不能覆盖critic模型的_attn_implementation和attn_implementation属性，只能去dp_critic.py中在执行output = self.critic_module前手动添加语句修改相应属性（setattr(self.critic_module.config, "_attn_implementation", "eager")），打印发现critic和actor走的底层模型并不相同，这可能是导致actor能够设置属性而critic不能设置的原因之一。
